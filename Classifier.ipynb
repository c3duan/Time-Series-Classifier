{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from RNN import RNN\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DATA_DIR = os.path.abspath(os.path.abspath('') + '/Aniyama_groundtruth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = [p for p in os.listdir(EVAL_DATA_DIR) if not p.startswith('.')]\n",
    "dataset = []\n",
    "for t in data_types:\n",
    "    type_dir = f'{EVAL_DATA_DIR}/{t}'\n",
    "    for file in os.listdir(type_dir):\n",
    "        filename = f'{type_dir}/{file}'\n",
    "        examples = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                tokens = line.rstrip().split(',')\n",
    "                power, anomaly = float(tokens[1]), int(tokens[2])\n",
    "                example = [power, anomaly]\n",
    "                examples.append(example)\n",
    "    examples = np.array(examples)\n",
    "    dataset.append(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for chunk in dataset:\n",
    "    size = chunk.shape[0]\n",
    "    new_size = (math.ceil(size / 10) * 10)\n",
    "    pad_size = new_size - size\n",
    "    padding = np.zeros((pad_size, 2))\n",
    "    new_chunk = np.vstack((chunk, padding)).reshape(new_size // 10, 10, 2)\n",
    "    for i in range(new_chunk.shape[0]):\n",
    "        data.append(new_chunk[i,:,0])\n",
    "        label = new_chunk[i,:,1].sum() > 0\n",
    "        labels.append(float(label))\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3429, 10), (3429,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_text = train_test_split(data, labels, test_size = 0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train, X_valid, X_test = torch.FloatTensor(X_train), torch.FloatTensor(X_train), torch.FloatTensor(X_train)\n",
    "y_train, y_valid, y_test = torch.FloatTensor(y_train), torch.FloatTensor(y_train), torch.FloatTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, data, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 3,\n",
    "    'drop_last': True,\n",
    "    'num_epochs': 100,\n",
    "    'encode_dim': 10,\n",
    "    'hidden_dim': 128,\n",
    "    'output_dim': 32,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.3,\n",
    "    'device': device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(X_train, y_train)\n",
    "train_loader = data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=config['shuffle'],\n",
    "    num_workers=config['num_workers'],\n",
    "    drop_last=config['drop_last']\n",
    ")\n",
    "\n",
    "validation_set = Dataset(X_valid, y_valid)\n",
    "validation_loader = data.DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=config['shuffle'],\n",
    "    num_workers=config['num_workers'],\n",
    "    drop_last=config['drop_last']\n",
    ")\n",
    "\n",
    "test_set = Dataset(X_test, y_test)\n",
    "test_loader = data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=config['shuffle'],\n",
    "    num_workers=config['num_workers'],\n",
    "    drop_last=config['drop_last']\n",
    ")\n",
    "\n",
    "model = RNN(\n",
    "    config['encode_dim'],\n",
    "    config['hidden_dim'],\n",
    "    config['output_dim'],\n",
    "    config['num_layers'],\n",
    "    config['dropout']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 207,904 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(config['device'])\n",
    "criterion = criterion.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_data, batch_labels in loader:\n",
    "        \n",
    "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch_data).squeeze(0)\n",
    "        \n",
    "        loss = criterion(predictions, batch_labels)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_data, batch_labels in loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            predictions = model(batch_data).squeeze(0)\n",
    "            \n",
    "            loss = criterion(predictions, batch_labels)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch_labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, device):\n",
    "    epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_data, batch_labels in loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            predictions = model(batch_data).squeeze(0)\n",
    "            acc = binary_accuracy(predictions, batch_labels)\n",
    "\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4e3f9ce5ea4b14a0e756c56909ff82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.698 | Train Acc: 47.56%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 50.34%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.71%\n",
      "\t Val. Loss: 0.699 |  Val. Acc: 48.78%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.697 | Train Acc: 49.07%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.73%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 51.76%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.05%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.696 | Train Acc: 49.85%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.00%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.41%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 48.05%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 50.73%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 49.56%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.71%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.49%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.07%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.42%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.29%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.51%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.51%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.51%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.68%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 52.49%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.07%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.73%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.88%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 49.12%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 48.63%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.88%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.80%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.34%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 52.10%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.27%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.68%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 49.76%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.44%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.32%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.88%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.59%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.93%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.98%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.44%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.63%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.41%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 49.85%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.73%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.00%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.03%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.10%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.692 | Train Acc: 52.00%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.00%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.42%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.10%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.692 | Train Acc: 53.12%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.32%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 51.12%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.54%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.73%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 50.63%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 48.83%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.56%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.41%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.95%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.15%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 52.88%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.80%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.15%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.20%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.76%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.71%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.15%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.54%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.81%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.54%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.63%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.66%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.37%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 52.15%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.27%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.63%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.71%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.76%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.44%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.37%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 52.39%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.85%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 52.20%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.07%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 49.95%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.63%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.03%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.73%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.90%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.66%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.29%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.59%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.34%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.27%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.93%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 51.17%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 52.44%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.61%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.46%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.34%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.20%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.39%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.71%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.42%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 52.44%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.49%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 50.49%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.29%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.51%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.22%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 52.98%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.90%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.00%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.73%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 48.58%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.46%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.95%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.90%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 50.93%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.80%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.03%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.692 | Train Acc: 52.00%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 50.68%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.692 | Train Acc: 51.71%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 52.15%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.66%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 52.69%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.690 | Train Acc: 53.22%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.07%\n",
      "\t-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.693 | Train Acc: 50.10%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 52.34%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.68%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.10%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.78%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 52.20%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.59%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.00%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.46%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.56%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.29%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 50.78%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.61%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 52.49%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 51.46%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.68%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.692 | Train Acc: 50.15%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 50.34%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.93%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 51.95%\n",
      "\t-------------------------------------------\n",
      "\tTrain Loss: 0.694 | Train Acc: 49.02%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.46%\n",
      "\t-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-0383edd6c396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-8fee139d93f6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm_notebook(range(config['num_epochs'])):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, config['device'])\n",
    "    valid_loss, valid_acc = evaluate(model, validation_loader, criterion, config['device'])\n",
    "    \n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'rnn-base-model.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print('\\t-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5009765625"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_loader, config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
